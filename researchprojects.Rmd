---
title: ''
output:
  html_document:
    toc: yes
    toc_float: yes
    collapsed: no
    number_sections: no
    toc_depth: 4
  pdf_document:
    toc: yes
    toc_depth: '4'
---


---


<p> Samples of past and in progress projects. </p>


---

## Philadelphia Public Schools

----

<div class="row">
<div class="col-sm-6">

#### Overview

In experiments where randomization is not feasible, propensity score matching helps to control for confounded relationships. Analysts working in this causal framework often run into a particular issue: sample size affects their ability to arrive at evenly matched samples. The problem is especially prevalent in observational studies which use administrative data. In a project with the <a href = "https://www.opendataphilly.org/" target = "_blank"> Philadelphia School District</a>, we arrive at better percent balance improvement in our causal model by trimming mis-represented groups. </br> *data private on github

#### Technical
Data cleaning: tidyverse </br>
Causal Inference: R (MatchIt)


</div>
<div class="col-sm-6">
```{r, out.width=200, echo=F}
knitr::include_graphics("images/philly.jpg")
```
</div>
</div>

<div class="row">
<div class="col-sm-6">

##### [project page](https://katjanewilson.github.io/PhillyTeacherRetention-Causal/)

  <a href="https://github.com/katjanewilson/PhillyTeacherRetention-Causal" target = "_blank"><i class="fa fa-github" aria-hidden="true"></i></a>
</div>
<div class="col-sm-6">
</div>
</div>

---


## COVID-19 Open Research

----

<div class="row">
<div class="col-sm-6">

#### Overview

Central to the debate of ethical algorithm design is a consideration of mis-classification costs for supervised learning methods. By building in asymmetric costs through sampling, machine learning engineers can take heed of policy makers' desired cost-ratios. This random forest algorithm takes asymmetric sampling into account when predicting death rates of coronavirus patients in South Korea using the <a href = "https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/" target = "_blank"> Kaggle </a>COVID-19 Open Research Dataset.</br> *course project for <a href = "https://apps.wharton.upenn.edu/syllabi/2020A/STAT474401/" target = "_blank"> Stat 974</a>

#### Technical

Data cleaning: tidyverse </br>
Random Forest: R (randomforest)

</div>
<div class="col-sm-4">
<script>
$(document).ready(function(){
    $('[data-toggle="popover"]').popover(); 
});
</script>

```{r echo=FALSE, message=FALSE}
#install.packages("randomForest")
#install.packages("kableExtra")
library(randomForest)
library(kableExtra)
library(tidyverse)
# load("data/coronavirus_data.Rdata")
# set.seed(222)
# rf <- randomForest(outcome~Sex + Age + Province + Source +
#                     Order, data=coronavirus_data,
#                    importance = TRUE,
#                    sampsize = c(21,12,26))
dataframe <- read.csv('data/COVID_random_forest_table.csv')
names(dataframe)[1] <- "Outcome1"
names(dataframe)[2] <- "Outcome2"
names(dataframe)[3] <- "Outcome3"
names(dataframe)[4] <- "Classification Error"
dataframe <- dataframe %>%
  mutate(`Classification Error` = round(`Classification Error`, 2))
dataframe <- dataframe %>%
  mutate(`Classification Error` = cell_spec(`Classification Error`, "html", color = ifelse(`Classification Error`>
                                                     0.5, "green",
                                                   "black"))) %>%
  mutate_if(is.numeric, function(x) {
    cell_spec(x, bold = T)
  }) %>%
  mutate(position = c("top", "top", "top"),
         text = c("84% Classification Accuracy", "54% Classification Accuracy", "87% Classification Accuracy"),
         name = c("0.16", "0.46", "0.13"))
dataframe <- dataframe %>%
  mutate(position2 = c("top", "top", "top"),
         text2 = c("In response to policy makers' request that a false negative in death classification be 20 times worse than a false positive release classification, deceased values are oversampled, at 21 out of 32, to avoid false positive releases. Due to a large imbalance in the outcome distribution, however, (less than 2% of patients die from the disease), a strict adherance to this cost ratio affects the ability to meet desired cost ratios in other categories.", "The intended cost ratio of a false negative death to a false positive isolation is set at 10:1. A larger ratio of 20:1 is achieved due to the high number of people isolated, and under-sampling from this parameter.", "A false negative release is about as risky as a false positive isolation, according to policy makers. The ratio intended was 2:1, and the actual result, with sampling, was 1.2:1."),
         name2 = c("Deceased", "Released", "Isolated"))
dataframe$`Classification Error` <- cell_spec(
  dataframe$name,
  popover = spec_popover(
    content = dataframe$text,
    title = NULL,  
    position = dataframe$position
  ))
dataframe$`Confusion Table`<- cell_spec(
  dataframe$name2,
  popover = spec_popover(
    content = dataframe$text2,
    title = NULL,  
    position = dataframe$position2
  ))
dataframe <- dataframe %>%
  select(`Confusion Table`, Outcome1, Outcome2, Outcome3, `Classification Error`)
kable(dataframe, escape = FALSE, caption = "<strong>Hover over the confusion table</strong> to learn more about how sampling affects cost ratios.") %>%
  kable_styling("striped", full_width = FALSE)


```
</div>
</div>

<div class="row">
<div class="col-sm-6">

##### [project page](https://katjanewilson.github.io/CORD-Random-Forest/)

  <a href="https://github.com/katjanewilson/CORD-Random-Forest" target = "_blank"><i class="fa fa-github" aria-hidden="true"></i></a>
</div>
<div class="col-sm-6">
</div>
</div>

---


## Los Angeles Census Tracts

----

<div class="row">
<div class="col-sm-6">

#### Overview

A hard to swallow assumption in the linear model framework is that a particular feature will change at the same rate across all values for a given variable. General Additive Models (GAM) give analysts the opportunity to identify nonlinear changes between covariates and the dependent variable. These nonparametric models can suffer from poor interpretability, however, and strong assumptions will have to be made regarding the data generation process. In a project using census data in the city of Los Angeles, GAM models identify outlier trends in homeless individuals, yet they also reveal limitations of the loess smoother at upper boundaries of data.</br> *course project for <a href = "https://apps.wharton.upenn.edu/syllabi/2020A/STAT474401/" target = "_blank"> Stat 974</a>

#### Technical
Data cleaning: tidyverse </br>
General Additive Models: mgc, mgcViz and gam


</div>
<div class="col-sm-4">
```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(DataExplorer)
library(gridExtra)

#inspect data
load("data/NewHomeless.rdata")
homeless <- homeless

#count the missing data values
# sapply(homeless, function(x) sum(is.na(x)))

# #inspect missing values
# missing <- homeless %>%
#   filter(is.na(PropVacant))

#impute the missing data
homeless[is.na(homeless)] = 0

#cleanign up the mis coded
# Adjust values of PropMinority mis-coded as "100" instead of "1.0"
library(dplyr)
homeless <- mutate_at(homeless, vars(PropMinority), list(~ ifelse( . > 1.0, 1.0, .)))

# Convert values of "Pct" variables to be in decimal scale
homeless <- mutate_at(homeless, vars(PctResidential:PCTIndustrial), list(~ .*.01))

# Recode four NAs in PropVacant as zero owing to how few residential dwellings exist in
# those tracts
homeless <- mutate_at(homeless, vars(PropVacant), list(~ ifelse(is.na(.)==T, 0, .)))

#recode the industrial
homeless <- homeless %>%
  mutate(Industrial = ifelse(PCTIndustrial >0, 1, 0))

####################
#Step 2: Univariate Statistics

library(DataExplorer)
#pairs
# pairs(homeless)
# #correlation
# plot_correlation(homeless, type = 'continuous','Review.Date')

#Street Total

par(mfrow = c(1,2))
p1 <- ggplot(aes(StreetTotal), data = homeless) +
  geom_histogram(binwidth = 6) +
  geom_vline(xintercept = mean(homeless$StreetTotal), size = 0.8)
p2 <- ggplot(aes(MedianIncome), data = homeless) +
  geom_histogram(bins = 50) +
  geom_vline(xintercept = mean(homeless$MedianIncome), colour = "pink", size = 0.8)
#Prop Vacant
p3 <- homeless %>%
  ggplot(aes(PropVacant)) +
  geom_histogram(bins = 40) +
  geom_vline(xintercept = mean(homeless$PropVacant), colour = "pink")
#Prop Minority
p4 <- homeless %>%
  ggplot(aes(PropMinority)) +
  geom_histogram(bins = 50) +
  geom_vline(xintercept = mean(homeless$PropMinority), colour = "pink")
#Pct Residential
p5 <- homeless %>%
  ggplot(aes(PctResidential)) +
  geom_histogram(bins = 50) +
  geom_vline(xintercept = mean(homeless$PctResidential), colour = "pink")
#PctCommercial
p6 <- homeless %>%
  ggplot(aes(PctCommercial)) +
  geom_histogram(bins = 50) +
  geom_vline(xintercept = mean(homeless$PctCommercial), colour = "pink")
#PctIndustrial
p7 <- homeless %>%
  ggplot(aes(PCTIndustrial)) +
  geom_histogram(bins = 50) +
  geom_vline(xintercept = mean(homeless$PCTIndustrial), colour = "pink")
library(gridExtra)
# grid.arrange(p1, p2, p3, p4, p5, p6, p7, nrow = 3)

#set seed at 4000
# Bring in data and create initial training and test datasets
set.seed(4000)
df <- homeless
index <- sample(1:nrow(df), (nrow(df))/2, replace = F) 
Train <- na.omit(df[index,]) # Training data
Test <- na.omit(df[-index,]) # Test data
##coming to the right model:
library(leaps)
All1 <- regsubsets(StreetTotal ~ MedianIncome + PropMinority +
                     PropVacant+
                     PctResidential+
                     PCTIndustrial +
                     PctCommercial +
                     Industrial, data = Train)
# All1
# summary(All1)

Model2 <- lm(StreetTotal ~  
               PropVacant+
               Industrial +
               (PropVacant:Industrial), data = Train)
# summary(Model2)

## Generalization error ##
# Specify a model on the testing data 
Model2 <- lm(StreetTotal ~  
               PropVacant+
               Industrial +
               (PropVacant:Industrial), data = Test)
# summary(Model2)
#estimate the fitted values on the testing data, and the generalization error
preds <- predict(Model2, newdata = Test) # New fitted values derived from the test data
GE <- var(Test$StreetTotal - preds) # Estimate of generalization error (the variance of the residuals when 
# GE
# sqrt(GE)
# Bootstrap a confidence interval for the generalization error 
bootstrap_genError <-
  function(x) {
    bootstrapped_genErrors <- 0
    for (i in 1:1000) {
      index <- sample(1:nrow(x), nrow(x), replace = T)
      sample_Test <- x[index,]
      sample_Test_preds <- predict(Model2, newdata = sample_Test)
      bootstrapped_genErrors[i] <- var(sample_Test$StreetTotal - sample_Test_preds)
    }
    return(bootstrapped_genErrors)
  }

bootstrap_results_ge <- bootstrap_genError(Test)

# Check generalization error bootstrap results
# mean(bootstrap_results_ge, na.rm = T)
# summary(bootstrap_results_ge)
# hist(bootstrap_results_ge, breaks = 20) 
# qqnorm(bootstrap_results_ge)
# sd(bootstrap_results_ge)
# quantile(bootstrap_results_ge, probs = c(.025,.975))
# plot(density(bootstrap_results_ge))
# qqnorm(bootstrap_results_ge)
#use robust sandwich standard errors in the sandwich package
# install.packages("sandwich")
library(sandwich)
# vcovHC(Model2, type = "HC")
# sandwich_se <- diag(vcovHC(Model2, type = "HC1"))
# sqrt(sandwich_se)
#then can get the confidence interval limits of these
# coef(Model2) - 1.96*sandwich_se
# coef(Model2) + 1.96*sandwich_se

library(gam)
library(mgcv)
library(leaps)

homeless2 <- homeless %>%
  filter(MedianIncome > 60000 & PropVacant < .10)

##MODEL 1
out3 <- gam(StreetTotal ~
              s(PCTIndustrial) +
              s(PctCommercial) +
              s(PropVacant) +
              s(MedianIncome), data = homeless, family = gaussian)
# summary(out3)
par(mfrow=c(2,2))
# plot(out3, residual = T, cex = 1, pch = 19, shade = T, shade.col = "light blue",
#      col = "#FAD7A0")
library(mgcViz)
b<- getViz(out3)
# o <- plot( sm(b, 1) )
# o + l_fitLine(colour = "#FAD7A0") + l_rug(mapping = aes(x=x, y=y), alpha = 0.9) +
#   l_ciLine(mul = 5, colour = "light blue", linetype = 2) +
#   l_points(shape = 19, size = 1, alpha = 0.7, color = "light blue") + theme_classic()
# 

```
</div>
</div>

<div class="row">
<div class="col-sm-6">

##### [project page](https://katjanewilson.github.io/LosAngelesCensusTracts//)

  <a href="https://github.com/katjanewilson/LosAngelesCensusTracts" target = "_blank"><i class="fa fa-github" aria-hidden="true"></i></a>
</div>
<div class="col-sm-6">
</div>
</div>

---

## Spotify Web API

----

<div class="row">
<div class="col-sm-6">

#### Overview

  Data from the <a href = "https://developer.spotify.com/documentation/web-api/" target = "_blank"> Spotify API </a> are fodder for a few data journalism projects currently in the works. One project involves visualizing changing applause levels among an artist's studio and live recordings. With this jitter plot, built using the JavaScript library D3.js, a user can evaluate distances between points and visualize the "ceiling effect" of comparison data.
  
#### Technical
Web scraping: Python (BeautifulSoup) </br>
Data Cleaning: tidyverse </br>
Data visualization: D3.js

</div>
</div>

<div class="row">
<div class="col-sm-6">

  <a href="https://github.com/katjanewilson/SpotifyAPI-D3JS" target = "_blank"><i class="fa fa-github" aria-hidden="true"></i></a>
</div>
<div class="col-sm-6">

</div>
<div class="col-sm-6">

</style>

<div class="row" style="padding-top: 30px;">
<div class="col-sm-6">


<body id = "updates and motions">

<div class = "container updates">
 <div class = "row updates">
 <div id = "chart-area-updates2"></div>
  </div>
 <select id="selectButton"></select>
</div>
 <svg id = "1"></svg>
</div>



```{r echo=FALSE, message=FALSE}

# d3.csv("data/small_website.csv"), (function(data){
#    console.log(data);
# });
# 
# 
# 
# var w = 400;
# var h = 400;
# var barPadding = 1;
# var padding = 20;
# 
# var svg = d3.select("#chart-area-updates2").append("svg")
#    .attr("width", w)
#    .attr("height", h);
# var jitterWidth = 100;
# 
# var allGroup = d3.map(data, function(d){
#   return(d.artist_name)}).keys();
#   
# var yScale = d3.scaleLinear()
#   .domain([0, 1])
#   .range([300,1]);
#   
# var yAxis = d3.axisLeft()
#   .scale(yScale)
#   .ticks(5);
#   
# 
# d3.select("#selectButton")
#   .selectAll('myOptions')
#   .data(allGroup)
#   .enter()
#   .append('option')
#   .text(function(d) { return d; })
#   .attr("value", function(d) {return d;});
#     
# 
# 
# svg.selectAll("circle")
#   .data(data)
#   .enter()
#   .append("circle")
#   .style("opacity", '0.7')
#   .attr("cx", function(d){
#     return(150 - Math.random() *jitterWidth)})
#   .attr("cy", function(d) {
#     return yScale(d.mean_liveness);
#   })
#   .attr("r", function(d) {
#     return d.mean_liveness*10;
# })
#   .attr("fill", function(d){
#     if(d.live_marker2 == "live"){
#       return "#CCE5FF";
#     } else{
#       return "#000099";
#     }
#   });
#  
# 
#     
# 
# 
# d3.select("#selectButton")
# .on("change", function(){
#   
#    var selectedOption = d3.select(this).property("value");
#    
#   svg.selectAll("circle")
#   .data(data)
#   .transition()
#   .duration(800)
#   .filter(function(d){
#     return d.artist_name == selectedOption;
#   })
#   .attr("cx", 250)
#   .attr("cy", function(d) {
#     return d.mean_liveness * 450;
#   })
#   .attr("r", function(d) {
#     return d.mean_liveness*20;
# })
#   .attr("fill", function(d){
#     if(d.live_marker2 == "live"){
#       return "#CCE5FF";
#     } else{
#       return "#000099";
#     }
#   });
#   });  
# 
# d3.select("#clickingtag")
#   .on("click", function(){
# 
# svg.selectAll("circle")
#   .data(data)
#   .filter(function(d){
#     return d.artist_name == "Nirvana";
#   })
#   .attr("cx", 150)
#   .attr("cy", function(d) {
#     return d.mean_liveness * 450;
#   })
#   .attr("r", function(d) {
#     return d.mean_liveness*7;
# })
#   .attr("fill", function(d){
#     if(d.live_marker2 == "live"){
#       return "blue";
#     } else{
#       return "black";
#     }
#   });
#   });  
#   
#   
# svg.append("g")
#   .attr("class", "axis")
#   .attr("transform", "translate(" + padding + ",0)")
#   .call(yAxis);
# 
# ```
# 
# 
# <h3 id = "updates and motions2"> Studio and Live Applause </h3>
# 
# <body id = "updates and motions">
# 
# <div class = "container updates">
#  <div class = "row updates">
#  <div id = "chart-area-updates2"></div>
#   </div>
#  <select id="selectButton"></select>
# </div>
#  <p id = "clickingtag"> Choose an artist </p>
#  <svg id = "1"></svg>
# </div>
# 
# 
# ```{r setupupdate00}
# knitr::opts_chunk$set(
# 	echo = TRUE,
# 	message = FALSE,
# 	warning = FALSE
# )
# d3.csv("data/small_website.csv"), (function(data){
#    console.log(data);
# });
# 
# 
# 
# var w = 400;
# var h = 400;
# var barPadding = 1;
# var padding = 20;
# 
# var svg = d3.select("#chart-area-updates2").append("svg")
#    .attr("width", w)
#    .attr("height", h);
# var jitterWidth = 100;
# 
# var allGroup = d3.map(data, function(d){
#   return(d.artist_name)}).keys();
#   
# var yScale = d3.scaleLinear()
#   .domain([0, 1])
#   .range([300,1]);
#   
# var yAxis = d3.axisLeft()
#   .scale(yScale)
#   .ticks(5);
#   
# 
# d3.select("#selectButton")
#   .selectAll('myOptions')
#   .data(allGroup)
#   .enter()
#   .append('option')
#   .text(function(d) { return d; })
#   .attr("value", function(d) {return d;});
#     
# 
# 
# svg.selectAll("circle")
#   .data(data)
#   .enter()
#   .append("circle")
#   .style("opacity", '0.7')
#   .attr("cx", function(d){
#     return(150 - Math.random() *jitterWidth)})
#   .attr("cy", function(d) {
#     return yScale(d.mean_liveness);
#   })
#   .attr("r", function(d) {
#     return d.mean_liveness*10;
# })
#   .attr("fill", function(d){
#     if(d.live_marker2 == "live"){
#       return "#CCE5FF";
#     } else{
#       return "#000099";
#     }
#   });
#  
# 
#     
# 
# 
# d3.select("#selectButton")
# .on("change", function(){
#   
#    var selectedOption = d3.select(this).property("value");
#    
#   svg.selectAll("circle")
#   .data(data)
#   .transition()
#   .duration(800)
#   .filter(function(d){
#     return d.artist_name == selectedOption;
#   })
#   .attr("cx", 250)
#   .attr("cy", function(d) {
#     return d.mean_liveness * 450;
#   })
#   .attr("r", function(d) {
#     return d.mean_liveness*20;
# })
#   .attr("fill", function(d){
#     if(d.live_marker2 == "live"){
#       return "#CCE5FF";
#     } else{
#       return "#000099";
#     }
#   });
#   });  
# 
# d3.select("#clickingtag")
#   .on("click", function(){
# 
# svg.selectAll("circle")
#   .data(data)
#   .filter(function(d){
#     return d.artist_name == "Nirvana";
#   })
#   .attr("cx", 150)
#   .attr("cy", function(d) {
#     return d.mean_liveness * 450;
#   })
#   .attr("r", function(d) {
#     return d.mean_liveness*7;
# })
#   .attr("fill", function(d){
#     if(d.live_marker2 == "live"){
#       return "blue";
#     } else{
#       return "black";
#     }
#   });
#   });  
#   
#   
# svg.append("g")
#   .attr("class", "axis")
#   .attr("transform", "translate(" + padding + ",0)")
#   .call(yAxis);

library(r2d3)
library(tidyverse)

r2d3(data = read.csv("data/small_website.csv"), d3_version = 4, script = "website_js.js")

```

</div>
</div>

---


## Growth Mindset Intervention

----

<div class="row">
<div class="col-sm-6">

#### Overview

Dual Enrollment programs offer high school students the opportunity to explore college curricula and earn college credit before graduation. With a large number of students participating in these programs each year in the state of Texas, researchers have the comfort of a large sample size and the opportunity to ethically randomize interventions. We evaluate how changes in growth mindset influence sense of belonging, course completion, and inclination to pursue STEM fields, for a large cohort (~3,000) of pre-calculus students.

#### Technical

Data cleaning: tidyverse </br>
Text Analysis: tidytext

</div>
<div class="col-sm-6">
```{r, out.width=200, echo=F}
knitr::include_graphics("images/onramps.png")
```
</div>
</div>

<div class="row">
<div class="col-sm-6">

  <a href="https://github.com/katjanewilson/OnRamps_Evaluation" target = "_blank"><i class="fa fa-github" aria-hidden="true"></i></a>
</div>
<div class="col-sm-6">
</div>
</div>
